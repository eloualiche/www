[{"id":0,"href":"/www/blog/posts/2024-03-TAQ-Consolidated-Data/","title":"Processing TAQ Consolidated Data","section":"Blog","content":"For a recent revision, I have had to dig into TAQ data. This is something relatively new to me, but I had to figure out some statistics out of the consolidated trade data. I did not want to use SAS because I do not have a PC running windows, and SAS for linux is very painful. So here is a method for getting annoying statistics from annoyingly large data.\nThe goal was to estimate the order imbalance volatility on the U.S. stock market. Order imbalance is defined as the difference between volume bought and sold scaled by the total volume exchanged:1\nOIBNUM: the number of buyer-initiated trades less the number of seller-initiated trades on day t. OIBSH: the buyer-initiated shares purchased less the seller-initiated shares sold on day t. OIBDOL: the buyer-initiated dollars paid less the seller-initiated dollars received on day t. It is too onerous (memory) to process everything at once, the goal of this guide is to show how to parallelize the process of working with this data stock by stock.\nData # For this I worked with the Trade and Quote (TAQ) data. Specifically, I downloaded consolidated trades data which is available from 1993 to 2014 directly from WRDS.2\nI downloaded the data year by year which took a little while given the size of each individual year (a gzipped extract in 2008 is above 26Gb and expands to 300Gb). Once downloaded, I reuploaded all the data to a s3 bucket where I have easy access to it.\nThe data includes a symbol for the stock id (there is a match table to permno on WRDS), a price, and a size.\nPreprocessing # I will process one year of data at a time \u0026mdash; technically, the algorithm for estimating whether a trade is a buy or sell could suffer from this, but I think the error is minimal here.\nThe preprocessing happens using a standard shell (bash or zsh).\nDefining variables # It is important to define a few variables for the shell\nThe relevant year: DATEY=2006 The number of processors available: nprocs=128 The memory available mem_alloc=512 (for example if you have 512G of ram available) A directory with lots of space (~1Tb) where you can expand and work with the data: TAQ_HOME=\u0026quot;/big_space_disk/\u0026quot; Download and expand the data # First I download the data from the s3 bucket using s5cmd as:3\n$ input_file=\u0026#34;TAQ_name_$DATEY\u0026#34; $ s5cmd cp s3://my-bucket/TAQ/$input_file.csv.gz $TAQ_HOME/ Then I expand the data; I take advantage of multicore expansion using pigz.\nFirst I peak at the data to check that the data has the correct columns and that the year is correct:\n$ pigz -cd | head -n 5 SYMBOL,DATE,TIME,PRICE,SIZE,G127,CORR,COND,EX A,2006-01-03,8:00:07,33.29,30000,0,0,U,T A,2006-01-03,8:08:01,33.29,8300,0,0,T,T A,2006-01-03,8:17:28,33.29,8600,0,0,T,T A,2006-01-03,9:30:22,33.4,96200,40,0,,N Once I have confirmed that I am looking at the right thing I expand the whole file:\n$ pigz -dc $TAQ_HOME/$input_file.csv.gz \u0026gt; $TAQ_HOME/taq_select.csv.gz For efficiency (untested), I have actually use the binaries from xsv4 to select only the columns from the file that I needed. Simply replace the previous command with:\npigz -cd $TAQ_HOME/$input_file.csv.gz | \\ xsv select \u0026#34;SYMBOL,DATE,TIME,PRICE,SIZE\u0026#34; \u0026gt; $TAQ_HOME/taq_select.csv Note that this is fairly slow and can take upwards of 20 minutes even reading the file from the ram disk.\nSort the data # In theory the data coming out of WRDS are already sorted by \u0026ldquo;SYMBOL\u0026rdquo;. But the strategy relies heavily on this step being accurate and it is always a good thing to learn how to use the very fancy unix sort function.\nWe want to split the data in chunks, one chunk for each symbol. It is a lot easier to do once the data is ordered by symbol!\nFirst we pipe the data without the header, then we use sort based on the first column and allow for parallel processing. The one thing to watch for is memory usage (this will eat pretty much of all your memory and get your job killed if you don\u0026rsquo;t watch it). So we define an upper bound for memory usage. Given the memory allocation defined above, we restrict sort to only use 80% of it:\nmem_for_sort=$(echo \u0026#34;${mem_alloc%G} * 0.8 / 1\u0026#34; | bc) # bc is the bash calculator /usr/bin/time tail -n +2 $TAQ_HOME/taq_select.csv | \\ sort -t, -k1,1 --buffer-size=\u0026#34;${mem_for_sort}G\u0026#34; --parallel=$nprocs --temporary-directory=$TAQ_HOME \u0026gt; $TAQ_HOME/taq_sorted.csv This can take a while. I have waited close to one hour for the largest files (128 cores, 490Gb of memory allocated).\nNote: you can clean up the non sorted file to save a little bit of space at this stage rm $TAQ_HOME/taq_select.csv\nSplit the data # Last of the preprocessing, we split the data in chunks: one chunk for each symbol/stock. Given the sorted nature of the data, this is a straightforward (I only spent a day figuring it out using chatGPT) application of awk\nFirst we make some room for the chunks by giving them their own directory\nTAQ_CHUNKS=\u0026#34;$TAQ_HOME/chunks/\u0026#34; mkdir -p $TAQ_CHUNKS Then we process the whole thing using awk\ncat $TAQ_HOME/taq_sorted.csv | \\ awk -v chunkDir=\u0026#34;$TAQ_CHUNKS\u0026#34; -F, \u0026#39; { if (last != $1) { if (last != \u0026#34;\u0026#34;) close(chunkDir \u0026#34;/chunk_\u0026#34; last \u0026#34;.csv\u0026#34;); last = $1; } print \u0026gt; (chunkDir \u0026#34;/chunk_\u0026#34; $1 \u0026#34;.csv\u0026#34;); }\u0026#39; The script reads the whole file line by line; it checks the first column (the \u0026ldquo;SYMBOL\u0026rdquo; column), if it is equal to the first column of the previous line, it appends the line to the file, if not it moves to create a new file. The created files are named based on the first column.\nThis process is sequential and can be quite slow (close to one hour for the largest file).\nProcessing of stock specific statistics in julia # We have close to 10,000 files in $TAQ_CHUNKS ready to be read one by one and processed. For this we are going to do some standard data processing in julia \u0026mdash; I compute OIB here, but this would for everything else that is at the stock-level.\nIf you have access to multiple cores, it makes sense to process this in paralle. It is fairly easy to implement; you still need to be careful not to trigger segfaults though.\nPreamble # My preamble is pretty standard and only use basic julia DataFrame stuff.\nusing CSV using DataFrames, DataFramesMeta using Dates, PeriodicalDates using Pipe: @pipe import ShiftedArrays: lag using Statistics import StatsBase: std @info Threads.nthreads() # usefule for parallelism later on # To ingest the command line parameter I pass to the script datey = ARGS[1] TAQ_CHUNKS = ARGS[2] # TAQ_CHUNKS=\u0026#34;/scratch.global/eloualic/taq/chunks\u0026#34; We are going to process each symbol one by one and store the statistics in a table.\nfile_list=readdir(TAQ_CHUNKS); n_files = length(file_list) df_oib_vol_array = Vector{Union{DataFrame, Nothing}}(nothing, n_files); Threads.@threads for i_f = 1:n_files # read the file for one stock df_taq_symbol = ingest_file(\u0026#34;$TAQ_CHUNKS/$file_in\u0026#34;; verbose=verbose) # create the trade sign flag df_taq_symbol = create_trade_sign!(df_taq_symbol) # create the order imbalance statistic df_oib = create_oib(df_taq_symbol) # get the volatility of the order imbalance df_oib_vol_array[i_f] = create_oib_vol(df_oib) end The most important step is how we read the file. Since this is happening inside a parallel loop, we need to make sure CSV.read only happens on a single thread.\nfunction read_file(file_in::AbstractString; verbose=false) df_taq_symbol = CSV.read(file_in, DataFrame, header=false, ntasks=1); rename!(df_taq_symbol, [:symbol, :date, :time, :price, :size]); end Next we follow Chordia et al. to estimate whether the trade is a buy or a sell order. Basically we compare the current price to previous prices (up to a lag of 5). This is the step that is the slowest when working with the whole dataset at a time.\nfunction create_trade_sign!(df_taq_symbol::DataFrame) @transform!(groupby(df_taq_symbol, :symbol), :l1_price=lag(:price), :l2_price=lag(:price, 2), :l3_price=lag(:price, 3), :l4_price=lag(:price, 4), :l5_price=lag(:price, 5)); @rtransform! df_taq_symbol @passmissing :trd_sgn = :price \u0026gt; :l1_price ? 1 : :price \u0026lt; :l1_price ? -1 : :price \u0026gt; :l2_price ? 1 : :price \u0026lt; :l2_price ? -1 : :price \u0026gt; :l3_price ? 1 : :price \u0026lt; :l3_price ? -1 : :price \u0026gt; :l4_price ? 1 : :price \u0026lt; :l4_price ? -1 : :price \u0026gt; :l5_price ? 1 : :price \u0026lt; :l5_price ? -1 : missing return df_taq_symbol end The other two functions are fairly straightforward and not particularly interesting\nfunction create_oib(df) #; to::TimerOutput=to) symbol_var=df[1,:symbol] dropmissing!(df, :trd_sgn) nrow(df)==0 \u0026amp;\u0026amp; return DataFrame(symbol=symbol_var, date=missing) # some do not have valid signed trades df_oib = @combine(groupby(df, [:date,:trd_sgn]), :oib_shr_sign=sum(:size), :oib_num_sign=length(:size)) |\u0026gt; (d -\u0026gt; @transform!(groupby(d, :date), :oib_shr_tot=sum(:oib_shr_sign), :oib_num_tot=sum(:oib_num_sign)) ) @transform!(groupby(df_oib, :date), :oib_shr_ratio=sum(:trd_sgn .* :oib_shr_sign) ./:oib_shr_tot, :oib_num_ratio=sum(:trd_sgn .* :oib_num_sign) ./:oib_num_tot, :symbol=symbol_var) return df_oib end function create_oib_vol(df) #; to::TimerOutput=to) symbol_var=df[1,:symbol] dropmissing!(df, :date) nrow(df)==0 \u0026amp;\u0026amp; return DataFrame(symbol=symbol_var, datem=missing, oib_shr_vol=missing, oib_num_vol=missing) df_oib_vol = unique(@rselect(df, :date, :datem=MonthlyDate(:date), :symbol, :oib_shr_ratio, :oib_num_ratio)) |\u0026gt; (d -\u0026gt; @combine(groupby(d, [:symbol, :datem]), :oib_shr_vol=std(:oib_shr_ratio), :oib_num_vol=std(:oib_num_ratio)) ) return df_oib_vol end I don\u0026rsquo;t run the code interactively (see the arguments passed above). The command I pass reads:\n$ julia -t $nprocs import_taq_chunks.jl $DATEY $TAQ_CHUNKS \u0026amp;\u0026gt; import_taq_chunks.log.jl SLURM Specifics # If you are running on the job on a cluster with the slurm scheduler, I have attached the commands I have used to make my life easier.\nChordia, Roll, Subrahmanyam (2002): Order imbalance, liquidity, and market returns, Journal of Financial Economics: 65\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nI could not find the relevant postgres database for the product, so I processed everything through webqueries. The product is taq_common, library is taq, and file is ct.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\ns5cmd is faster than s3cmd, but in the grand scheme of things here this is not going to matter very much.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nI tried installing the more recent qsv but could not compile it properly on the Minnesota Supercomputing Institute MSI cluster.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n"},{"id":1,"href":"/www/blog/posts/2024-03-ST-and-Julia/","title":"Sublime Text and Julia IDE","section":"Blog","content":"Sometimes, I am asked how to set up a simple julia IDE for people who are already using Sublime Text. It is simple in theory but many things can go wrong, so I thought this document could be useful.\nI try to provide step by step instructions as much as possible. Feel free to email me if some of the steps were not as clear as you would have liked.\nN.B.\nThere might be better ways to have a julia IDE (see VSCode with LSP). I happen to like Sublime, it is simpler and faster than VSCode (but I am also an emacs person). Also note that the same thing would apply for R (just change a few paths here and there). This guide was written on 2024-03-xx for Sublime Text 4 and julia 1.10.1\n1. Prerequisite # The \u0026ldquo;required\u0026rdquo; software for this guide are: julia, Sublime Text, and a Terminal application called iterm2.\nFirst, I would recommend to install (optional but recommended) a nicer terminal application than the built-in macos terminal:\nDownload iterm2.\nThis is not strictly necessary but iterm2 has some nice feature that make it work nicely with the Sublime setup here. There are other good terminal applications for macos (e.g. kitty) but I am not as familiar with those. Open the iterm2 application which should be in the application folder (note I will use the words iterm2 and the terminal interchangeably (technically they are not quite the same thing but it won\u0026rsquo;t matter). Download Sublime Text 4 and follow the instructions to install.\nDownload julia and follow the instructions to install.\nAs I am writing this, you can use juliaup to install julia by copying and pasting in your terminal the following: curl -fsSL https://install.julialang.org | sh This allows you to keep your version of julia up to date (this might be a bad thing if you have old code and are not templating packages) Or (simpler imho) download the latest current stable release and install it depending on your platform For example for macos with Apple Silicon download the dmg and install it in your application folder 2. Getting julia to work! # 2.1 Basic installation # If julia is in your application folder, there are two ways to start a julia session.\nDouble click on the julia icon: this opens the macos terminal application (not iterm2) and starts julia Open the terminal (I am assuming iterm2 from now on) and type or paste the path of the julia application binary. For julia 1.10.1 the binary will likely at /Applications/Julia-1.10.app/Contents/Resources/julia/bin/julia If you are working with version 1.XX (where XX is a different number) the path will likely be /Applications/Julia-1.XX.app/Contents/Resources/julia/bin/julia 2.2 Adding julia to your path # Do you need it? # If you have installed julia using juliaup (see section 1 above), the julia application was automatically added to your PATH.1 I believe this means the next step is not necessary. To see whether you should skip the next step type julia inside of iterm2. If julia opens, you are in business, skip 2.2 and go to section 3. If there is some error or julia does not open, follow the instructions in the next section 2.2.\nAdding julia to the path # Typing the full julia path can get annoying pretty quickly, so we will add the directory to the PATH for the terminal (technically for the shell). To make this permanent we are going to edit a special file which is read everytime you start your terminal.\nOn macos the file is .zshrc since the default shell is zsh; this is a configuration file. On other machine the file might be .bashrc if your default shell is bash.\nTo edit this file we need a text editor. Any will do; for example you could use textedit and enter the following at the terminal: open -a TextEdit ~/.zshrc This tells textedit to open the zsh configuration file which is located on your home directory under ~2 You could also use Sublime Text that you just opened; the following should work if it installed properly in the Applications folder: /Applications/Sublime\\ Text.app/Contents/SharedSupport/bin/subl ~/.zshrc\nNow we need to edit this file by adding julia to the path. We can simply add this line:\nexport PATH=$PATH:/Applications/Julia-1.10.app/Contents/Resources/julia/bin This concatenates your path with what was already in it ($PATH) and the directory where the julia binary is. If you are working with a version different than 1.10 then adjust the line for 1.XX as: export PATH=$PATH:/Applications/Julia-1.XX.app/Contents/Resources/julia/bin\nSave the file. Close the window. Restart the terminal (iterm2). Now you should be able to start julia in the terminal by typing julia from anywhere.\n3. Getting Sublime Text to play nice with julia # Sublime text is an editor that relies on packages for added functionality. So next, we will download the necessary packages to get a setup that will let you work with julia from sublime text.\n3.1. Installing Sublime packages # First install the package that can install other packages: that is package control. The instructions to install it are:\nOpen the command palette (cmd+shift+p on macos) Type Install Package Control, press enter Then install the Julia-sublime package similary:\nOpen the command palette (cmd+shift+p on macos) Type Package Control: Install, press enter Then a dropdown menu will show and type julia to search for the julia package, type enter to install it. Last install the SendCode package\nOpen the command palette (cmd+shift+p on macos) Type Package Control: Install, press enter Then a dropdown menu will show and type SendCode to search for the SendCode package, type enter to install it. 3.2 Setting up Sublime packages # Settings of SendCode Open the settings of sendcode: open the command palette (cmd+shift+p on macos), type SendCode settings, and enter You should be dropped into a new window with two files (one on each side). Edit the one on the right which is the one with your personal settings (the one on the left has the general settings). Its name is SendCode.sublime-settings. If you are working with iterm2 copy and paste the following: { \u0026#34;auto_advance\u0026#34;: true, \u0026#34;prog\u0026#34;: \u0026#34;iterm\u0026#34;, \u0026#34;julia\u0026#34;: { \u0026#34;bracketed_paste_mode\u0026#34;: true, \u0026#34;prog\u0026#34;: \u0026#34;iterm\u0026#34;, }, \u0026#34;r\u0026#34;: { \u0026#34;bracketed_paste_mode\u0026#34;: true, \u0026#34;prog\u0026#34;: \u0026#34;iterm\u0026#34;, }, } Keybindings SendCode Open the settings of sendcode: open the command palette (cmd+shift+p on macos), type SendCode key bindings, and enter You should be dropped into a new window with two files (one on each side). Edit the one on the right which is the one with your personal settings (the one on the left has the general settings). This time do not remove existing keybindings as they might be useful stuff. You should add the following to make sure sending lines or highlighted content to be sent to iterm2. { \u0026#34;keys\u0026#34;: [\u0026#34;super+enter\u0026#34;], \u0026#34;command\u0026#34;: \u0026#34;send_code\u0026#34;, \u0026#34;context\u0026#34;: [{ \u0026#34;key\u0026#34;: \u0026#34;selector\u0026#34;, \u0026#34;operator\u0026#34;: \u0026#34;equal\u0026#34;, \u0026#34;operand\u0026#34;: \u0026#34;source\u0026#34; } ] }, { \u0026#34;keys\u0026#34;: [\u0026#34;ctrl+enter\u0026#34;], \u0026#34;command\u0026#34;: \u0026#34;send_code\u0026#34;, \u0026#34;context\u0026#34;: [{ \u0026#34;key\u0026#34;: \u0026#34;selector\u0026#34;, \u0026#34;operator\u0026#34;: \u0026#34;equal\u0026#34;, \u0026#34;operand\u0026#34;: \u0026#34;source\u0026#34; } ] }, Feel free to change the key bindings for example if you want Cmd-s to send to iterm2 you would replace \u0026ldquo;super+enter\u0026rdquo; by \u0026ldquo;super+s\u0026rdquo;. You should be good to go. Create a test.jl file. Open a terminal session and start julia on the side. select 1+1 in sublime text and press Cmd+Enter at the same time. It should be sending to the terminal.\nYour path is essentially a set of places where the terminal looks for software. So if the directory of julia gets added to the path, the terminal is aware of the julia application and you can type directly $ julia.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n~ represents what is known as your home directory. The actual address is /Users/yourname/. You can find it by doing in the terminal: cd ~ to change directory to the home directory, and then pwd to print the directory you are currently in.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n"},{"id":2,"href":"/www/blog/posts/2021-10-Kleibergen-Paap/","title":"Kleibergen-Paap F-statistics in stata and julia","section":"Blog","content":"This was a joint effort with Matthieu Gomez who created and maintain the high-dimensional fixed effect regression package for julia FixedEffectModels.jl. Valentin Haddad and myself uncovered this bug while working on our project on passive investing and market competition.\nThis post describes how the Kleibergen-Paap first-stage F-statistics can be misleading when using ivreg2 and ivreghdfe in stata.\nIn julia the FixedEffectModels package deals with this type of regressions and usually mimicks stata. We found the issue was due to a specific case in the Kleibergen-Paap original paper and were able to patch it here. We find that the issue appears with instruments interacted with fixed effects.\ntl;dr; We found potential mistakes in stata\u0026rsquo;s ivreg2 and ivreghdfe for special cases. We found a way to fix it in julia\u0026rsquo;s FixedEffectModels; stata developers should look into patching it.\nFramework # We consider a model with two groups indexed by \\(k \\in {1,2}\\). We are interested in the following regression: $$ \\begin{align} Y_{i,k} \u0026amp;= \\alpha_k + \\beta_1 X_{i,1} + \\beta_2 X_{i,2} + \\varepsilon_i \\\\ X_{i,k} \u0026amp;= \\delta_k + \\gamma_k Z_{i,k} + u_{i,k} \\end{align} $$ The \\(X\\)s are endogenous regressors and the \\(Z\\)s are instruments. We are interested in a two-stage-least-squares regression of \\(Y\\) on \\(X\\) using the exogenous variation of the \\(Z\\).\nSetting up the problem # We set up the problem in julia (v1.6.0) and provide a minimal reproducible example. We start by setting up the packages and creating a small dataset (20 rows) with two identifiers.\nFirst we import the packages (sorry no checkpointing here apart from for the core package).\nIf you want to run this in isolation you can create a new directory and run it from there with julia --project=. and then activate the directory using Pkg; Pkg.activate(\u0026quot;.\u0026quot;); Pkg.instantiate()\nimport Pkg; Pkg.add(name=\u0026#34;FixedEffectModels\u0026#34;, version=\u0026#34;1.4.2\u0026#34;); # this new version has the fix Pkg.add(name=\u0026#34;Vcov\u0026#34;, version=\u0026#34;0.4.2\u0026#34;); # this new version has the fix # if you want to run the version without the fix (mimicks stata) # Pkg.add(name=\u0026#34;Vcov\u0026#34;, version=\u0026#34;1.4.0\u0026#34;); # Pkg.add(name=\u0026#34;Vcov\u0026#34;, version=\u0026#34;0.4.0\u0026#34;); # if you want to run the version without the fix (mimicks stata) Pkg.add([\u0026#34;Revise\u0026#34;, \u0026#34;CategoricalArrays\u0026#34;, \u0026#34;DataFrames\u0026#34;, \u0026#34;Random\u0026#34;, \u0026#34;CSV\u0026#34;, \u0026#34;Suppressor\u0026#34;]); Pkg.add([\u0026#34;Plots\u0026#34;, \u0026#34;PGFPlotsX\u0026#34;, \u0026#34;LaTeXStrings\u0026#34;]); Pkg.add([\u0026#34;FixedEffectModels\u0026#34;, \u0026#34;RegressionTables\u0026#34;]); Pkg.add(\u0026#34;RCall\u0026#34;); Pkg.add(url=\u0026#34;https://github.com/jmboehm/StataCall.jl#master\u0026#34;); And then load them for our session\nusing Printf using Revise, Suppressor using CategoricalArrays, DataFrames, CSV, Random using Plots, LaTeXStrings; pgfplotsx(size=(600, 400)); using FixedEffectModels using StataCall, RegressionTables using RCall Generating the dataset # The following function generates the dataset. We introduce a small \\(\\epsilon \\) to add noise to the instrument. This will be useful later as we show that the problem fails locally but not once we perturb the data.\nfunction gen_df(ϵ; seed::Int=107, N_rows=10) Random.seed!(seed) N_id = 2; i = 1 df_example = DataFrame() for i in 1:N_id obs = 1:N_rows id = i .* Int.(ones(N_rows)) Z = rand(N_rows) X = Z + 0.5 .* rand(N_rows) Y = 3 .* Z + 0.3 .* rand(N_rows) df_tmp = DataFrame(obs=obs, id = id, Z = Z, X = X, Y = Y) df_example = vcat(df_example, df_tmp) end df_example1 = unstack(select(df_example, :obs, :id, :X), [:obs, :id, :X], :id, :X, renamecols=x-\u0026gt;Symbol(:X, Int(x))) df_example2 = unstack(select(df_example, :obs, :id, :Z), [:obs, :id, :Z], :id, :Z, renamecols=x-\u0026gt;Symbol(:Z, Int(x))) df_example = innerjoin(df_example, select(df_example1, :obs, :id, :X1, :X2), on = [:obs, :id]) df_example = innerjoin(df_example, select(df_example2, :obs, :id, :Z1, :Z2), on = [:obs, :id]) df_example[ ismissing.(df_example.X1), :X1] .= 0.0; df_example[ ismissing.(df_example.X2), :X2] .= 0.0; df_example[ ismissing.(df_example.Z1), :Z1] .= 0.0; df_example[ ismissing.(df_example.Z2), :Z2] .= 0.0; transform!(df_example, :id =\u0026gt; categorical =\u0026gt; :id) transform!(df_example, :Z1 =\u0026gt; (x-\u0026gt; x .+ ϵ .* rand(N_id*N_rows)) =\u0026gt; :Z1eps) transform!(df_example, :Z2 =\u0026gt; (x-\u0026gt; x .+ ϵ .* rand(N_id*N_rows)) =\u0026gt; :Z2eps) return(df_example) end; And you can generate and view the data as follows:\n$ df_example = gen_df(0.01, seed=107, N_rows=10); $ show(df_example, display_size=(19, 200)) 20×11 DataFrame Row │ obs id Z X Y X1 X2 Z1 Z2 Z1eps Z2eps │ Int64 Cat… Float64 Float64 Float64 Float64? Float64? Float64? Float64? Float64 Float64 ─────┼──────────────────────────────────────────────────────────────────────────────────────────────────────────── 1 │ 1 1 0.119228 0.478444 0.587155 0.478444 0.0 0.119228 0.0 0.121428 3.31773e-5 2 │ 2 1 0.26591 0.568968 0.938078 0.568968 0.0 0.26591 0.0 0.272448 0.000209308 3 │ 3 1 0.745445 1.01965 2.25625 1.01965 0.0 0.745445 0.0 0.749129 0.00955548 4 │ 4 1 0.174331 0.208681 0.746424 0.208681 0.0 0.174331 0.0 0.178839 0.00739797 5 │ 5 1 0.370533 0.759199 1.20848 0.759199 0.0 0.370533 0.0 0.375502 0.00863599 ⋮ │ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ 16 │ 6 2 0.674156 0.891977 2.10047 0.0 0.891977 0.0 0.674156 0.00662125 0.676856 17 │ 7 2 0.568036 0.787749 1.9021 0.0 0.787749 0.0 0.568036 0.00465503 0.57743 18 │ 8 2 0.935972 1.03212 3.03463 0.0 1.03212 0.0 0.935972 0.00264424 0.939824 19 │ 9 2 0.719989 0.927739 2.37524 0.0 0.927739 0.0 0.719989 0.00676318 0.729181 20 │ 10 2 0.840836 0.973702 2.75698 0.0 0.973702 0.0 0.840836 0.00371209 0.849899 10 rows omitted Running the regression: julia # We run two sets of regressions. These ones have the bug fix and should run correctly.\nThe first regression is the one described above. The second one uses the noisy instruments rather than the original instruments. The noisy instruments are defined as $$ Z_{i,k}^{\\epsilon} = Z_{i,k} + \\epsilon \\cdot \\text{Uniform} $$ What is interesting is how the Kleibergen-Paap statistics stay close as we perturb the data slightly.\nr1 = reg(df_example, @formula(Y ~ fe(id) + (X1 + X2 ~ Z1 + Z2) ), Vcov.robust() ); r2 = reg(df_example, @formula(Y ~ fe(id) + (X1 + X2 ~ Z1eps + Z2eps) ), Vcov.robust() ); And to view the results\n$ regtable(r1,r2; regression_statistics = [:f_kp]) --------------------------------------------- Y ------------------- (1) (2) --------------------------------------------- X1 3.415*** 3.395*** (0.771) (0.763) X2 4.129*** 4.135*** (0.474) (0.475) --------------------------------------------- id Fixed Effects Yes Yes --------------------------------------------- First-stage F statistic 12.231 12.432 --------------------------------------------- To check the smoothness of the statistic and that the knife-edge case has actually been fixed, we look at how the F-stat varies for different values of the perturbation.\nϵ_vec = range(-0.025, 0.025, length=25) F_vec = similar(ϵ_vec) iter_ϵ = 1 for iter_ϵ in 1:length(ϵ_vec) df_example = gen_df(ϵ_vec[iter_ϵ], seed=107, N_rows=10) r_tmp = reg(df_example, @formula(Y ~ fe(id) + (X1 + X2 ~ Z1eps + Z2eps) ), Vcov.robust() ) F_vec[iter_ϵ] = r_tmp.F_kp end plot(ϵ_vec, F_vec, xlabel=L\u0026#34;$\\epsilon$ Perturbation of Instrument Z\u0026#34;, ylabel=\u0026#34;Kleibergen-Paap first stage F-stat\u0026#34;, legend=false, dpi=300); plot!([0.0], seriestype=\u0026#34;vline\u0026#34;, color=:black) Running the regression: Stata # We use the StataCall package to run commands in stata. To set it up you need to make sure that julia has access to your path, see the repo for more details on how to do it for your system.\nIt might be easier to save the file and then load directly in stata.\ndf_example = gen_df(0.01, seed=107, N_rows=10); CSV.write(\u0026#34;./stata_data.csv\u0026#34;, df_example); If you go the StataCall route, you will simply pass the regression commands as strings:\ndfOut = StataCall.stataCall( [\u0026#34;ivreg2 Y (X1 X2 = Z1 Z2) id, robust\u0026#34;; \u0026#34;gen F = e(rkf)\u0026#34;; \u0026#34;ivreg2 Y (X1 X2 = Z1eps Z2eps) id, robust\u0026#34;; \u0026#34;gen Feps = e(rkf)\u0026#34;; ], df_example, true, true, true); # replace the last argument by false to see the stata-log Float64(dfOut.F[1]), Float64(dfOut.Feps[1]); # returns the F-stats We can confirm that the results are not smooth\n$ msg = \u0026#34;\\nF-statistics estimated with ivreg2 and exact instruments: F = \u0026#34; * @sprintf(\u0026#34;%.2f\u0026#34;, dfOut.F[1]) * \u0026#34;\\nF-statistics estimated with ivreg2 and ϵ-perturbed instruments: F = \u0026#34; * @sprintf(\u0026#34;%.2f\u0026#34;, dfOut.Feps[1]); $ @info msg ┌ Info: │ F-statistics estimated with ivreg2 and exact instruments: F = 0.00 └ F-statistics estimated with ivreg2 and ϵ-perturbed instruments: F = 12.43 We find that the result is 0 for the case with the actual instrument, which is not the expected result. We also checked whether the trick of perturbing the instruments might help and confirm the results above found in the julia code: the F-stat is 12.43 which corresponds to the desired estimate.\nWe try using ivreghdfe which suffers from similar issues. Note that ivreghdfe at least throws a warning (about collinearity) and does not return a number for the statistic. The peturbation still works well.\ndfOut = StataCall.stataCall( [\u0026#34;ivreghdfe Y (X1 X2 = Z1 Z2), absorb(id) robust\u0026#34;; \u0026#34;gen F = e(rkf)\u0026#34;; \u0026#34;ivreghdfe Y (X1 X2 = Z1eps Z2eps), absorb(id) robust\u0026#34;; \u0026#34;gen Feps = e(rkf)\u0026#34;; ], df_example, true, true, true); # replace the last argument by false to see the stata-log Fout = map(x-\u0026gt; ismissing(x) ? string(x) : @sprintf(\u0026#34;%.2f\u0026#34;, x), [dfOut.F[1], Float64(dfOut.Feps[1])] ) # returns the F-stats Same as before:\n$ msg = \u0026#34;\\nF-statistics estimated with ivreghdfe and exact instruments: F = \u0026#34; * Fout[1] * \u0026#34;\\nF-statistics estimated with ivreghdfe and ϵ-perturbed instruments: F = \u0026#34; * Fout[2] $ @info msg ┌ Info: │ F-statistics estimated with ivreghdfe and exact instruments: F = 0.00 └ F-statistics estimated with ivreghdfe and ϵ-perturbed instruments: F = 12.43 The fix to the covariance estimator only consisted of a few lines to handle a special case. You can see the commit here.\nIdeally this would be implemented in the stata code as well.\nRunning the regression: R # We use the RCall package to run commands in R within julia just as with StataCall (to run it make sure that your R environment is in the path, and of course install the relevant packages).\n# Load all relevant libraries to R (make sure they are installed in your R) @rput df_example @suppress begin R\u0026#34;\u0026#34;\u0026#34; library(data.table, verbose=T) library(lfe, verbose=T) library(fixest, verbose=T) library(texreg, verbose=T) dt_reg = data.table(df_example); dt_reg[, id := as.factor(id)]; \u0026#34;\u0026#34;\u0026#34; end RObject{VecSxp} It is hard to make sense of the results of felm here (note that I included id clustering, otherwise the estimation throws an error probably because of an zero inversion).\n# @suppress begin R\u0026#34;\u0026#34;\u0026#34; est_felm \u0026lt;- felm(Y ~ X1 + X2 | id | (X1 + X2 ~ Z1 + Z2) | id, dt_reg) est_felm_eps \u0026lt;- felm(Y ~ X1 + X2 | id | (X1 + X2 ~ Z1eps + Z2eps) | id, dt_reg) # res_s = screenreg(list(est_felm, est_felm_eps), include.fstatistic = T) \u0026#34;\u0026#34;\u0026#34; end @rget res_s print(res_s) The feols function detects collinearity in X1 and X2 which I do not understand, and stops there. Edit: this seems to gives us the correct results here.\n@suppress begin R\u0026#34;\u0026#34;\u0026#34; est_feols \u0026lt;- feols(Y ~ 1 | id | X1 + X2 ~ Z1 + Z2, dt_reg) est_feols_eps \u0026lt;- feols(Y ~ 1 | id | X1 + X2 ~ Z1eps + Z2eps, dt_reg) \u0026#34;\u0026#34;\u0026#34; # or run tests R\u0026#34;\u0026#34;\u0026#34; fitstat(est_feols, \u0026#34;ivf\u0026#34;) fitstat(est_feols_eps, \u0026#34;ivf\u0026#34;) fitstat(est_feols, \u0026#34;ivwald\u0026#34;) fitstat(est_feols_eps, \u0026#34;ivwald\u0026#34;) \u0026#34;\u0026#34;\u0026#34; end R\u0026#34;\u0026#34;\u0026#34; print(est_feols) print(est_feols_eps) \u0026#34;\u0026#34;\u0026#34; Perhaps we are missing other relevant packages that would take care of this issue (AER or estimatr). However these two are the fastest and most prevalent when dealing with relatively high-dimensional data. At this point it is not clear how to deal with this type of regressions in R.\n"}]